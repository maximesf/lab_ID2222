{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef72bc5",
   "metadata": {},
   "source": [
    "# Homework 2: Discovery of Frequent Itemsets and Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a28b7",
   "metadata": {},
   "source": [
    "The minimum support (MIN_SUPPORT = 800) and minimum confidence (MIN_CONFIDENCE = 0.5) are the two thresholds that govern the outcome of the analysis: only itemsets appearing at least 800 times are considered frequent, and only rules with a predictive accuracy of 50% or more are considered valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96fd8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# --- Algo parameters ---\n",
    "MIN_SUPPORT = 800\n",
    "MIN_CONFIDENCE = 0.5\n",
    "current_dir = os.getcwd() \n",
    "file_path = os.path.join(current_dir, 'T10I4D100K.dat')\n",
    "output_file_path = os.path.join(current_dir, 'apriori_results.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbda41",
   "metadata": {},
   "source": [
    "The provided code implements the classic Apriori algorithm, which is justified by its efficiency in pruning the vast candidate space using the Apriori Principle. \n",
    "\n",
    "**1. The Apriori Principle**\n",
    "The foundation of this code's efficiency lies in the Apriori principle:\n",
    "\n",
    "**2. Methods:**\n",
    "\n",
    "load_transactions(self, file_path)\n",
    "Justification: The transactional data must be efficiently loaded and represented.We converted each transaction to a set ({item1, item2, ...}) \n",
    "\n",
    "Technical Choice: Using sets enabled extremely fast checking of the subset relationship (A <= t) in the get_itemset_support method.\n",
    "If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "\n",
    "get_itemset_support(self, itemsets): \n",
    "This method performs the counting phaseâ€”the most computationally intensive step in each iteration. It must count the occurrence of every candidate itemset $C_k$ across all transactions.\n",
    "\n",
    "generate_candidates(self, frequent_itemsets, k):\n",
    " This method implements the Apriori-Gen function. It generates candidates $C_k$ (of size $k$) by joining two frequent itemsets $L_{k-1}$ (of size $k-1$) only if they share the first $k-2$ items.\n",
    "\n",
    "find_frequent_itemsets(self): After finding $L_{k-1}$ and generating $C_k$, we only need to keep the transactions that contain at least one candidate from $C_k$\n",
    "\n",
    "generate_rules(self): This method generates all possible implication rules ($X \\to Y$) from every frequent itemset $L_k$ where $k \\geq 2$.Confidence Calculation: The core justification here is the formula for confidence:$$Confidence(X \\to Y) = \\frac{Support(X \\cup Y)}{Support(X)}$$In the code, this is support_itemset / support_sub. Since $X \\cup Y$ is the original frequent itemset being processed, its support (support_itemset) is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8366cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apriori:\n",
    "    def __init__(self, min_support, min_confidence):\n",
    "        # Minimum support count required for an itemset to be considered frequent\n",
    "        self.min_support = min_support\n",
    "        # List to store all transactions (each transaction is a set of items)\n",
    "        self.transactions = []\n",
    "        self.frequent_itemsets = {} #storing all found itemsets with the format {itemset_tuple: support_count}\n",
    "        # Minimum confidence required for an association rule to be considered valid\n",
    "        self.min_confidence = min_confidence\n",
    "        #storing generated association rules \n",
    "        self.rules =[] # Format [(sub_set, res, confidence), ...] e.g: {704} -> {825}, Confidence = 0.6143\n",
    "\n",
    "    def load_transactions(self, file_path):\n",
    "        \"\"\"Read dataset and store as list of sets.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.transactions = [set(line.strip().split()) for line in f]\n",
    "        print(f\"Loaded {len(self.transactions)} transactions.\")\n",
    "\n",
    "    def get_itemset_support(self, itemsets):\n",
    "        \"\"\"returns how many transactions contain each itemset \"\"\"\n",
    "        support_counts = defaultdict(int)\n",
    "        for t in self.transactions:\n",
    "            for itemset in itemsets:\n",
    "                if set(itemset) <= t:  # itemset is subset of transaction\n",
    "                    support_counts[itemset] += 1\n",
    "        return support_counts\n",
    "\n",
    "    def generate_candidates(self, frequent_itemsets, k):\n",
    "        \"\"\"Generate candidate itemsets of size k+1 from previous itemsets\"\"\"\n",
    "        candidates = set()\n",
    "        frequent_list = list(frequent_itemsets.keys())\n",
    "        for i in range(len(frequent_list)):\n",
    "            for j in range(i+1, len(frequent_list)):\n",
    "                itemset1, itemset2 = frequent_list[i], frequent_list[j]\n",
    "                if itemset1[:k-1] == itemset2[:k-1]:\n",
    "                    candidate = tuple(sorted(set(itemset1) | set(itemset2)))\n",
    "                    candidates.add(candidate)\n",
    "        return list(candidates)\n",
    "\n",
    "    def filter_itemsets_by_support(self, itemset_support):\n",
    "        \"\"\"Filters the candidate itemsets based on the minimum support count.\"\"\"\n",
    "        return {itemset: count for itemset, count in itemset_support.items() if count >= self.min_support}\n",
    "\n",
    "    def find_frequent_itemsets(self):\n",
    "        \"\"\"Main Apriori algorithm.\"\"\"\n",
    "        print(f\"Running Apriori with min_support = {self.min_support}\")\n",
    "        # Single-item candidates\n",
    "        items = {item for t in self.transactions for item in t}\n",
    "        # (C1)\n",
    "        current_itemsets = [tuple([item]) for item in sorted(items)]\n",
    "        k = 1\n",
    "\n",
    "        while current_itemsets:\n",
    "            #1. (Ck)\n",
    "            support_counts = self.get_itemset_support(current_itemsets)\n",
    "            #2. Filtering : (Lk)\n",
    "            frequent_itemsets_k = self.filter_itemsets_by_support(support_counts)\n",
    "            print(f\"Found {len(frequent_itemsets_k)} frequent itemsets of size {k}\")\n",
    "            # 3. Storing the frequent itemsets\n",
    "            self.frequent_itemsets.update(frequent_itemsets_k)\n",
    "\n",
    "            # next iteration (k+1)\n",
    "            k += 1\n",
    "            # 4. Generate candidates for the next size (Ck+1)\n",
    "            current_itemsets = self.generate_candidates(frequent_itemsets_k, k-1)\n",
    "\n",
    "            if k > 2 and current_itemsets:\n",
    "                self.transactions = [t for t in self.transactions if any(set(c) <= t for c in current_itemsets)]\n",
    "    def write_results_to_file(self, output_file_path, rule_count):\n",
    "        \"\"\"Writes frequent itemsets (with support) and association rules to a file.\"\"\"\n",
    "        print(f\"\\nWriting results to {output_file_path}\")\n",
    "        \n",
    "        # Grouping frequent itemsets by size (k)\n",
    "        itemsets_by_k = defaultdict(list)\n",
    "        for itemset, support in self.frequent_itemsets.items():\n",
    "            itemsets_by_k[len(itemset)].append((itemset, support))\n",
    "\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(\"Frequent Itemsets (by size k)\\n\\n\")\n",
    "\n",
    "            # Write Frequent Itemsets\n",
    "            sorted_ks = sorted(itemsets_by_k.keys())\n",
    "            for k in sorted_ks:\n",
    "                f.write(f\"--- k = {k} ---\\n\")\n",
    "                sorted_itemsets = sorted(itemsets_by_k[k], key=lambda x: x[0])\n",
    "                for itemset, support in sorted_itemsets:\n",
    "                    itemset_str = \"{\" + \", \".join(itemset) + \"}\"\n",
    "                    f.write(f\"{itemset_str}: Support Count = {support}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"=====================================\\n\\n\")\n",
    "            f.write(\"=== Association Rules ===\\n\\n\")\n",
    "\n",
    "            # Writing Association Rules\n",
    "            f.write(f\"Total rules generated (Confidence >= {self.min_confidence}): {rule_count}\\n\\n\")\n",
    "            if not self.rules:\n",
    "                f.write(\"No association rules found that satisfy min_confidence.\\n\")\n",
    "            else:\n",
    "                # Iterate through stored rules (antecedent, consequent, confidence)\n",
    "                for antecedent, consequent, confidence in self.rules:\n",
    "                    ante_str = \"{\" + \", \".join(sorted(list(antecedent))) + \"}\"\n",
    "                    cons_str = \"{\" + \", \".join(sorted(list(consequent))) + \"}\"\n",
    "                    f.write(f\"{ante_str} -> {cons_str}\\n\")\n",
    "            \n",
    "            f.write(\"\\n=========================\\n\")\n",
    "        print(\"Results successfully written to file.\")\n",
    "    \n",
    "    def generate_rules(self):\n",
    "        cmp = 0 # Counter for rules generated\n",
    "        for itemset in self.frequent_itemsets:\n",
    "            if len(itemset)<2:\n",
    "                continue #rule made of 2 items at least\n",
    "            # Generate all non-empty proper subsets of the current itemset\n",
    "            subsets = [set(x) for i in range(1, len(itemset)) for x in combinations(itemset, i)]\n",
    "            for sub in subsets:\n",
    "                # Consequent is the rest of the itemset\n",
    "                res = set(itemset) - sub\n",
    "                if not res:\n",
    "                    continue\n",
    "                support_itemset = self.frequent_itemsets[itemset]\n",
    "                # Look up support for the antecedent (sub). 0 if not found.\n",
    "                support_sub = self.frequent_itemsets.get(tuple(sorted(sub)), 0)#because sub is a set and the keys of the dictionary are tuples e.g ('32',)\n",
    "                if support_sub == 0:\n",
    "                    continue\n",
    "                confidence = support_itemset / support_sub\n",
    "\n",
    "                if confidence >= self.min_confidence:\n",
    "                    self.rules.append((sub,res, confidence)) #each rule is a tuple + its associated confidence\n",
    "                    cmp +=1\n",
    "        return cmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb4e6c",
   "metadata": {},
   "source": [
    "We instantiate the Apriori object and proceed with the two main stages of association rule discovery. First, apriori.load_transactions(file_path) reads and parses the input file, loading the data into memory as a list of sets. Second, apriori.find_frequent_itemsets() executes the iterative Apriori process, reporting the number of frequent itemsets found at each size $k$. Finally, rule_count = apriori.generate_rules() finds all valid association rules, reporting that rule_count total rules met the confidence criterion. The subsequent console output displays a sample of the frequent itemsets (L-k) and the resulting association rules ($X \\to Y$) found in the dataset, confirming the successful discovery phase.\n",
    "\n",
    "The method apriori.write_results_to_file(output_file_path, rule_count) provides comprehensive output by systematically writing all frequent itemsets (grouped by size $k$) and all valid association rules to the specified file, ensuring all results are preserved for external analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee089fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 transactions.\n",
      "Running Apriori with min_support = 800\n",
      "Found 443 frequent itemsets of size 1\n",
      "Found 44 frequent itemsets of size 2\n",
      "Found 7 frequent itemsets of size 3\n",
      "\n",
      "Frequent Itemsets:\n",
      "('240',): 1399\n",
      "('25',): 1395\n",
      "('274',): 2628\n",
      "('368',): 7828\n",
      "('448',): 1370\n",
      "('52',): 1983\n",
      "('538',): 3982\n",
      "('561',): 2783\n",
      "('630',): 1523\n",
      "('687',): 1762\n",
      "('775',): 3771\n",
      "('825',): 3085\n",
      "('834',): 1373\n",
      "('120',): 4973\n",
      "('205',): 3605\n",
      "('39',): 4258\n",
      "('401',): 3667\n",
      "('581',): 2943\n",
      "('704',): 1794\n",
      "('814',): 1672\n",
      "('35',): 1984\n",
      "('674',): 2527\n",
      "('712',): 845\n",
      "('733',): 1141\n",
      "('854',): 2847\n",
      "('950',): 1463\n",
      "('422',): 1255\n",
      "('449',): 1890\n",
      "('857',): 1588\n",
      "('895',): 3385\n",
      "('937',): 4681\n",
      "('964',): 1518\n",
      "('229',): 2281\n",
      "('283',): 4082\n",
      "('294',): 1445\n",
      "('352',): 902\n",
      "('381',): 2959\n",
      "('708',): 1090\n",
      "('738',): 2129\n",
      "('766',): 6265\n",
      "('853',): 1804\n",
      "('883',): 4902\n",
      "('966',): 3921\n",
      "('978',): 1141\n",
      "('104',): 1158\n",
      "('143',): 1417\n",
      "('569',): 2835\n",
      "('620',): 2100\n",
      "('798',): 3103\n",
      "('185',): 1529\n",
      "('214',): 1893\n",
      "('350',): 3069\n",
      "('529',): 7057\n",
      "('658',): 1881\n",
      "('682',): 4132\n",
      "('7',): 997\n",
      "('782',): 2767\n",
      "('809',): 2163\n",
      "('947',): 3690\n",
      "('970',): 2086\n",
      "('227',): 1818\n",
      "('390',): 2685\n",
      "('192',): 2004\n",
      "('208',): 1483\n",
      "('279',): 3014\n",
      "('280',): 2108\n",
      "('496',): 1428\n",
      "('530',): 1263\n",
      "('597',): 2883\n",
      "('618',): 1337\n",
      "('675',): 2976\n",
      "('71',): 3507\n",
      "('720',): 3864\n",
      "('855',): 939\n",
      "('914',): 4037\n",
      "('932',): 1786\n",
      "('183',): 3883\n",
      "('193',): 925\n",
      "('217',): 5375\n",
      "('276',): 2479\n",
      "('277',): 982\n",
      "('474',): 815\n",
      "('626',): 874\n",
      "('653',): 2634\n",
      "('706',): 1923\n",
      "('878',): 2047\n",
      "('161',): 2320\n",
      "('175',): 2791\n",
      "('177',): 4629\n",
      "('424',): 1448\n",
      "('490',): 1066\n",
      "('571',): 2902\n",
      "('623',): 1845\n",
      "('795',): 3361\n",
      "('910',): 1695\n",
      "('960',): 2732\n",
      "('125',): 1287\n",
      "('130',): 1711\n",
      "('839',): 854\n",
      "('392',): 2420\n",
      "('461',): 1498\n",
      "('801',): 835\n",
      "('862',): 3649\n",
      "('27',): 2165\n",
      "('78',): 2471\n",
      "('900',): 1165\n",
      "('921',): 2425\n",
      "('147',): 1383\n",
      "('411',): 2047\n",
      "('572',): 1589\n",
      "('579',): 2164\n",
      "('778',): 2514\n",
      "('803',): 2237\n",
      "('266',): 1022\n",
      "('290',): 1793\n",
      "('458',): 1124\n",
      "('523',): 2244\n",
      "('614',): 3134\n",
      "('888',): 3686\n",
      "('944',): 2794\n",
      "('969',): 849\n",
      "('204',): 2174\n",
      "('334',): 2146\n",
      "('43',): 1721\n",
      "('480',): 2309\n",
      "('513',): 1287\n",
      "('70',): 2411\n",
      "('874',): 2237\n",
      "('151',): 2611\n",
      "('432',): 985\n",
      "('504',): 1296\n",
      "('830',): 841\n",
      "('890',): 1437\n",
      "('118',): 916\n",
      "('310',): 1390\n",
      "('388',): 938\n",
      "('419',): 5057\n",
      "('469',): 1502\n",
      "('484',): 971\n",
      "('722',): 5845\n",
      "('73',): 2179\n",
      "('810',): 1267\n",
      "('844',): 2814\n",
      "('846',): 1480\n",
      "('918',): 3012\n",
      "('967',): 1695\n",
      "('326',): 1488\n",
      "('403',): 1722\n",
      "('526',): 2793\n",
      "('774',): 2046\n",
      "('788',): 2386\n",
      "('789',): 4309\n",
      "('975',): 1764\n",
      "('116',): 2193\n",
      "('198',): 1461\n",
      "('201',): 1029\n",
      "('395',): 990\n",
      "('171',): 1097\n",
      "('541',): 3735\n",
      "('701',): 1283\n",
      "('805',): 1789\n",
      "('946',): 1350\n",
      "('471',): 2894\n",
      "('487',): 3135\n",
      "('631',): 2793\n",
      "('638',): 2288\n",
      "('640',): 932\n",
      "('678',): 1329\n",
      "('735',): 1689\n",
      "('780',): 2306\n",
      "('935',): 1742\n",
      "('17',): 1683\n",
      "('242',): 2325\n",
      "('758',): 2860\n",
      "('763',): 1862\n",
      "('956',): 3626\n",
      "('145',): 4559\n",
      "('385',): 1676\n",
      "('676',): 2717\n",
      "('790',): 1094\n",
      "('792',): 1306\n",
      "('885',): 3043\n",
      "('522',): 2725\n",
      "('617',): 2614\n",
      "('859',): 1242\n",
      "('12',): 3415\n",
      "('296',): 2210\n",
      "('354',): 5835\n",
      "('548',): 2843\n",
      "('684',): 5408\n",
      "('740',): 1632\n",
      "('841',): 1927\n",
      "('210',): 2009\n",
      "('346',): 3470\n",
      "('477',): 2462\n",
      "('605',): 1652\n",
      "('829',): 6810\n",
      "('884',): 1645\n",
      "('234',): 1416\n",
      "('355',): 958\n",
      "('460',): 4438\n",
      "('649',): 1292\n",
      "('746',): 1982\n",
      "('600',): 1192\n",
      "('157',): 1140\n",
      "('28',): 1454\n",
      "('742',): 953\n",
      "('115',): 1775\n",
      "('5',): 1094\n",
      "('517',): 1201\n",
      "('736',): 1470\n",
      "('744',): 2177\n",
      "('919',): 3710\n",
      "('196',): 2096\n",
      "('489',): 3420\n",
      "('494',): 5102\n",
      "('641',): 1494\n",
      "('673',): 1635\n",
      "('723',): 829\n",
      "('362',): 4388\n",
      "('591',): 1241\n",
      "('622',): 826\n",
      "('181',): 1235\n",
      "('31',): 1666\n",
      "('329',): 964\n",
      "('417',): 971\n",
      "('472',): 2125\n",
      "('573',): 1229\n",
      "('58',): 1330\n",
      "('628',): 1102\n",
      "('651',): 1288\n",
      "('111',): 1171\n",
      "('154',): 1447\n",
      "('168',): 1538\n",
      "('580',): 1667\n",
      "('632',): 1070\n",
      "('832',): 2062\n",
      "('871',): 2810\n",
      "('988',): 1164\n",
      "('585',): 856\n",
      "('72',): 2852\n",
      "('981',): 1542\n",
      "('10',): 1351\n",
      "('132',): 2641\n",
      "('464',): 848\n",
      "('21',): 2666\n",
      "('239',): 2742\n",
      "('32',): 4248\n",
      "('348',): 1226\n",
      "('54',): 2595\n",
      "('100',): 1749\n",
      "('500',): 1444\n",
      "('126',): 1075\n",
      "('319',): 1371\n",
      "('48',): 2472\n",
      "('639',): 1572\n",
      "('765',): 1705\n",
      "('521',): 1582\n",
      "('112',): 2680\n",
      "('140',): 2687\n",
      "('285',): 2600\n",
      "('387',): 2089\n",
      "('511',): 1015\n",
      "('594',): 1516\n",
      "('583',): 1389\n",
      "('606',): 2668\n",
      "('93',): 2777\n",
      "('236',): 2618\n",
      "('952',): 1574\n",
      "('593',): 2601\n",
      "('90',): 1875\n",
      "('941',): 1126\n",
      "('122',): 1081\n",
      "('718',): 1238\n",
      "('1',): 1535\n",
      "('423',): 1412\n",
      "('516',): 1544\n",
      "('415',): 877\n",
      "('6',): 2149\n",
      "('69',): 2370\n",
      "('797',): 2684\n",
      "('913',): 1939\n",
      "('980',): 899\n",
      "('577',): 1695\n",
      "('110',): 1801\n",
      "('509',): 3044\n",
      "('611',): 1444\n",
      "('995',): 1521\n",
      "('343',): 1599\n",
      "('447',): 863\n",
      "('527',): 1185\n",
      "('158',): 884\n",
      "('33',): 1460\n",
      "('336',): 1071\n",
      "('989',): 1289\n",
      "('574',): 1297\n",
      "('793',): 3063\n",
      "('97',): 1466\n",
      "('598',): 3219\n",
      "('427',): 1856\n",
      "('470',): 4137\n",
      "('37',): 1249\n",
      "('858',): 866\n",
      "('992',): 1116\n",
      "('55',): 1959\n",
      "('347',): 885\n",
      "('481',): 888\n",
      "('897',): 1935\n",
      "('95',): 841\n",
      "('224',): 919\n",
      "('275',): 1692\n",
      "('259',): 1522\n",
      "('51',): 1612\n",
      "('162',): 1450\n",
      "('378',): 1149\n",
      "('45',): 1728\n",
      "('534',): 1531\n",
      "('906',): 1444\n",
      "('912',): 1009\n",
      "('44',): 903\n",
      "('576',): 1337\n",
      "('642',): 830\n",
      "('879',): 865\n",
      "('96',): 975\n",
      "('18',): 813\n",
      "('373',): 2007\n",
      "('716',): 1199\n",
      "('546',): 1050\n",
      "('568',): 956\n",
      "('665',): 1297\n",
      "('785',): 947\n",
      "('963',): 1327\n",
      "('349',): 2041\n",
      "('197',): 1230\n",
      "('413',): 2637\n",
      "('749',): 1330\n",
      "('8',): 3090\n",
      "('823',): 1031\n",
      "('94',): 1201\n",
      "('982',): 1640\n",
      "('984',): 1756\n",
      "('515',): 1166\n",
      "('692',): 4993\n",
      "('694',): 2847\n",
      "('567',): 1102\n",
      "('57',): 2743\n",
      "('800',): 1916\n",
      "('812',): 1518\n",
      "('41',): 1353\n",
      "('414',): 1160\n",
      "('590',): 814\n",
      "('923',): 1753\n",
      "('377',): 1149\n",
      "('160',): 987\n",
      "('456',): 804\n",
      "('752',): 2578\n",
      "('991',): 1268\n",
      "('998',): 2713\n",
      "('899',): 1252\n",
      "('114',): 816\n",
      "('710',): 1044\n",
      "('867',): 1530\n",
      "('170',): 1203\n",
      "('438',): 4511\n",
      "('563',): 1065\n",
      "('705',): 888\n",
      "('357',): 1142\n",
      "('659',): 835\n",
      "('332',): 1861\n",
      "('361',): 1104\n",
      "('322',): 1154\n",
      "('928',): 1034\n",
      "('108',): 940\n",
      "('486',): 1547\n",
      "('75',): 3151\n",
      "('440',): 1943\n",
      "('38',): 2402\n",
      "('784',): 1257\n",
      "('265',): 1359\n",
      "('624',): 915\n",
      "('686',): 1495\n",
      "('943',): 821\n",
      "('540',): 1293\n",
      "('468',): 1089\n",
      "('663',): 2354\n",
      "('819',): 1257\n",
      "('886',): 3053\n",
      "('429',): 1037\n",
      "('843',): 1222\n",
      "('129',): 1547\n",
      "('578',): 1290\n",
      "('510',): 3281\n",
      "('68',): 1601\n",
      "('860',): 1255\n",
      "('318',): 812\n",
      "('4',): 1394\n",
      "('304',): 805\n",
      "('887',): 1671\n",
      "('309',): 1262\n",
      "('804',): 1315\n",
      "('325',): 1022\n",
      "('745',): 909\n",
      "('268',): 885\n",
      "('826',): 2022\n",
      "('394',): 1145\n",
      "('707',): 1354\n",
      "('838',): 953\n",
      "('105',): 1100\n",
      "('815',): 1358\n",
      "('948',): 1149\n",
      "('345',): 801\n",
      "('308',): 1402\n",
      "('661',): 2693\n",
      "('634',): 2492\n",
      "('351',): 1641\n",
      "('405',): 1525\n",
      "('688',): 1132\n",
      "('949',): 1414\n",
      "('163',): 1256\n",
      "('893',): 1947\n",
      "('335',): 1345\n",
      "('922',): 990\n",
      "('173',): 1080\n",
      "('203',): 861\n",
      "('258',): 1036\n",
      "('629',): 893\n",
      "('314',): 846\n",
      "('66',): 888\n",
      "('85',): 1555\n",
      "('450',): 2082\n",
      "('428',): 1021\n",
      "('550',): 1203\n",
      "('769',): 1622\n",
      "('80',): 826\n",
      "('558',): 957\n",
      "('608',): 999\n",
      "('507',): 950\n",
      "('554',): 1114\n",
      "('366',): 1031\n",
      "('689',): 817\n",
      "('820',): 1473\n",
      "('831',): 852\n",
      "('207',): 1214\n",
      "('39', '704'): 1107\n",
      "('704', '825'): 1102\n",
      "('39', '825'): 1187\n",
      "('529', '782'): 862\n",
      "('227', '390'): 1049\n",
      "('795', '853'): 806\n",
      "('571', '795'): 838\n",
      "('623', '795'): 805\n",
      "('392', '862'): 881\n",
      "('411', '803'): 826\n",
      "('290', '888'): 826\n",
      "('208', '969'): 806\n",
      "('208', '888'): 829\n",
      "('208', '290'): 803\n",
      "('888', '969'): 810\n",
      "('471', '678'): 810\n",
      "('789', '829'): 1194\n",
      "('392', '489'): 866\n",
      "('368', '829'): 1194\n",
      "('541', '72'): 846\n",
      "('598', '782'): 800\n",
      "('529', '598'): 943\n",
      "('283', '33'): 845\n",
      "('217', '346'): 1336\n",
      "('33', '346'): 844\n",
      "('217', '283'): 926\n",
      "('283', '346'): 910\n",
      "('217', '515'): 843\n",
      "('33', '515'): 824\n",
      "('217', '33'): 852\n",
      "('283', '515'): 835\n",
      "('346', '515'): 849\n",
      "('923', '947'): 859\n",
      "('438', '684'): 825\n",
      "('684', '70'): 860\n",
      "('684', '765'): 812\n",
      "('684', '819'): 800\n",
      "('368', '682'): 1193\n",
      "('368', '494'): 860\n",
      "('368', '692'): 928\n",
      "('390', '722'): 1042\n",
      "('675', '886'): 823\n",
      "('227', '722'): 995\n",
      "('471', '960'): 935\n",
      "('39', '704', '825'): 1035\n",
      "('283', '33', '346'): 802\n",
      "('217', '346', '515'): 809\n",
      "('217', '283', '346'): 827\n",
      "('217', '33', '346'): 802\n",
      "('283', '346', '515'): 806\n",
      "('227', '390', '722'): 907\n",
      "\n",
      "Generating associated rules to the frequent Itemsets found:\n",
      "Total rules found (confidence >= 0.5): 45\n",
      "{'704'} -> {'39'}\n",
      "{'704'} -> {'825'}\n",
      "{'227'} -> {'390'}\n",
      "{'208'} -> {'969'}\n",
      "{'969'} -> {'208'}\n",
      "{'208'} -> {'888'}\n",
      "{'208'} -> {'290'}\n",
      "{'969'} -> {'888'}\n",
      "{'678'} -> {'471'}\n",
      "{'33'} -> {'283'}\n",
      "{'33'} -> {'346'}\n",
      "{'515'} -> {'217'}\n",
      "{'33'} -> {'515'}\n",
      "{'515'} -> {'33'}\n",
      "{'33'} -> {'217'}\n",
      "{'515'} -> {'283'}\n",
      "{'515'} -> {'346'}\n",
      "{'819'} -> {'684'}\n",
      "{'227'} -> {'722'}\n",
      "{'704'} -> {'825', '39'}\n",
      "{'704', '39'} -> {'825'}\n",
      "{'825', '39'} -> {'704'}\n",
      "{'825', '704'} -> {'39'}\n",
      "{'33'} -> {'283', '346'}\n",
      "{'283', '33'} -> {'346'}\n",
      "{'283', '346'} -> {'33'}\n",
      "{'33', '346'} -> {'283'}\n",
      "{'515'} -> {'217', '346'}\n",
      "{'217', '346'} -> {'515'}\n",
      "{'217', '515'} -> {'346'}\n",
      "{'515', '346'} -> {'217'}\n",
      "{'283', '217'} -> {'346'}\n",
      "{'217', '346'} -> {'283'}\n",
      "{'283', '346'} -> {'217'}\n",
      "{'33'} -> {'217', '346'}\n",
      "{'217', '33'} -> {'346'}\n",
      "{'217', '346'} -> {'33'}\n",
      "{'33', '346'} -> {'217'}\n",
      "{'515'} -> {'283', '346'}\n",
      "{'283', '346'} -> {'515'}\n",
      "{'283', '515'} -> {'346'}\n",
      "{'515', '346'} -> {'283'}\n",
      "{'390', '227'} -> {'722'}\n",
      "{'227', '722'} -> {'390'}\n",
      "{'390', '722'} -> {'227'}\n",
      "\n",
      "Writing results to c:\\Users\\Mon pc\\Documents\\lab_ID2222\\lab2\\apriori_results.txt\n",
      "Results successfully written to file.\n"
     ]
    }
   ],
   "source": [
    "# Initializing and running Apriori\n",
    "apriori = Apriori(MIN_SUPPORT, MIN_CONFIDENCE)\n",
    "apriori.load_transactions(file_path)\n",
    "apriori.find_frequent_itemsets()\n",
    "\n",
    "# Generate rules and capture the count (called only once)\n",
    "rule_count = apriori.generate_rules()\n",
    "\n",
    "# Console Output: Frequent Itemsets\n",
    "print(\"\\nFrequent Itemsets:\")\n",
    "for itemset, support in apriori.frequent_itemsets.items():\n",
    "    print(f\"{itemset}: {support}\")\n",
    "\n",
    "# Console Output: Association Rules\n",
    "print(\"\\nGenerating associated rules to the frequent Itemsets found:\")\n",
    "print(f\"Total rules found (confidence >= {MIN_CONFIDENCE}): {rule_count}\")\n",
    "\n",
    "# Iterate and print rules (unpacking all 3, but only displaying 2 elements as requested)\n",
    "for antecedent, consequent, confidence in apriori.rules:\n",
    "    print(f\"{antecedent} -> {consequent}\")\n",
    "\n",
    "# Saving results to a file\n",
    "apriori.write_results_to_file(output_file_path, rule_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b41ab2",
   "metadata": {},
   "source": [
    "Following this, the plotting functions analyze the structure of the results. Specifically, plot_itemset_count_vs_k() visualizes the rapid drop in frequent itemset count as $k$ increases, which is a key characteristic of the Apriori algorithm's performance on sparse data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fe449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_itemset_count_vs_k(frequent_itemsets):\n",
    "    \"\"\"Plot 1: Bar chart showing the number of frequent itemsets for each size k.\"\"\"\n",
    "    \n",
    "    # Prepare data: Count itemsets by length\n",
    "    k_counts = defaultdict(int)\n",
    "    for itemset in frequent_itemsets.keys():\n",
    "        k_counts[len(itemset)] += 1\n",
    "    \n",
    "    df = pd.Series(k_counts).sort_index().rename_axis('Itemset Size (k)').reset_index(name='Count')\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(df['Itemset Size (k)'], df['Count'], color='skyblue')\n",
    "    plt.title('Number of Frequent Itemsets vs. Size (k)')\n",
    "    plt.xlabel('Itemset Size (k)')\n",
    "    plt.ylabel('Count of Frequent Itemsets')\n",
    "    plt.xticks(df['Itemset Size (k)'])\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig('plot_itemset_count_vs_k.png')\n",
    "    plt.close()\n",
    "    print(\"Plot 1: plot_itemset_count_vs_k.png saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa1de3",
   "metadata": {},
   "source": [
    "Similarly, plot_rules_by_length() summarizes the complexity of the rules found (e.g., $1 \\to 1$ rules are likely the most common). These plots are vital for understanding the underlying distribution and complexity of the discovered patterns within the transactional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b948602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rules_by_length(rules):\n",
    "    \"\"\"Plot : Bar chart showing the count of rules grouped by format (len(A) -> len(B)).\"\"\"\n",
    "    \n",
    "    # Prepare data: Format as \"len(A) -> len(B)\"\n",
    "    rule_formats = []\n",
    "    for antecedent, consequent, _ in rules:\n",
    "        format_str = f\"{len(antecedent)} -> {len(consequent)}\"\n",
    "        rule_formats.append(format_str)\n",
    "        \n",
    "    df = pd.Series(rule_formats).value_counts().sort_index().rename_axis('Rule Format').reset_index(name='Count')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df['Rule Format'], df['Count'], color='mediumseagreen')\n",
    "    plt.title('Count of Association Rules by Format (Antecedent $\\\\to$ Consequent Size)')\n",
    "    plt.xlabel('Rule Format (e.g., \"1 $\\\\to$ 1\")')\n",
    "    plt.ylabel('Count of Rules')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plot_rules_by_length.png')\n",
    "    plt.close()\n",
    "    print(\"Plot 3: plot_rules_by_length.png saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7987fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating visualizations...\n",
      "Plot 1: plot_itemset_count_vs_k.png saved.\n",
      "Plot 3: plot_rules_by_length.png saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Plots ---\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "if apriori.frequent_itemsets:\n",
    "    plot_itemset_count_vs_k(apriori.frequent_itemsets)\n",
    "       # plot_top_k1_items(apriori.frequent_itemsets, n=10)\n",
    "else:\n",
    "    print(\"No frequent itemsets found.\")\n",
    "        \n",
    "if apriori.rules:\n",
    "    plot_rules_by_length(apriori.rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114e09a",
   "metadata": {},
   "source": [
    "The following code represents an alternative implementation of the Apriori algorithm developed by another member of the group. In this section, we focus on analyzing the execution time of the algorithm and evaluating how it is affected by different parameters, including the support threshold, the confidence level, and the number of data lines used during mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time as t\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "class FrequentItem:\n",
    "    itemFound: np.matrix\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        print(\"Frequent Item Finder\")\n",
    "    \n",
    "    def findSingletons(self, set: np.matrix,threshold: int) -> list[any]:\n",
    "        temp = []\n",
    "        for subset in set:\n",
    "            temp = np.hstack((temp,np.unique(subset)))\n",
    "        uniqueItems = np.unique(temp)\n",
    "        items = []\n",
    "        for i,item in enumerate(uniqueItems):\n",
    "            sum = 0\n",
    "            for j,subset in enumerate(set):\n",
    "                idx = (subset == item)\n",
    "                sum += np.sum(idx)\n",
    "                if(sum/len(set)>=threshold):\n",
    "                        items.append(item)\n",
    "                        break\n",
    "        return items\n",
    "    \n",
    "    def buildCandidates(self, singletons: list[any], multiplons: np.matrix) -> list[any]:\n",
    "        candidates = []\n",
    "        for set in multiplons:\n",
    "            for single in singletons:\n",
    "                if(np.isin(single,set,invert=True)):\n",
    "                    if isinstance(set, np.integer):\n",
    "                        temp = [set, single]\n",
    "                        temp.sort()\n",
    "                        candidates.append(temp)\n",
    "                    else:\n",
    "                        temp = set\n",
    "                        temp = np.hstack((temp,single))\n",
    "                        temp.sort()\n",
    "                        candidates.append(temp)\n",
    "      \n",
    "        return np.unique(candidates,axis=0)\n",
    "    \n",
    "    #initialize frequentItem with singletons\n",
    "    #each subset of the initial working set should be a set and not a bag meaning that every subset contain an element exactly once\n",
    "    def getFrequentItemFinder(self,threshold:int,set: np.matrix,frequentItem : list , singletons : list[any], depth: int = 0) -> list[any]:\n",
    "        candidates = self.buildCandidates(singletons,frequentItem[depth])\n",
    "        #print(f\"candidates= {candidates}\")\n",
    "        depth +=1\n",
    "        items = []\n",
    "        for i,item in enumerate(candidates):\n",
    "            sum = 0\n",
    "            for j,subset in enumerate(set):\n",
    "                mask = np.isin(subset,item)\n",
    "                if(np.sum(mask)==depth+1):\n",
    "                    sum += 1\n",
    "            if(sum/len(set)>=threshold):\n",
    "                items.append(item)\n",
    "        #print(f\"items that are frequent= {items}\")\n",
    "        if(len(items)==0):\n",
    "            #print(\"exting recursive loop\")\n",
    "            #print(frequentItem)\n",
    "            self.itemFound = frequentItem\n",
    "            return frequentItem\n",
    "        else:\n",
    "            frequentItem.append(items)\n",
    "            #print(f\"resulting matrix= {frequentItem}\")\n",
    "            self.getFrequentItemFinder(threshold,set,frequentItem,singletons,depth)\n",
    "\n",
    "    def computeSupport(self, set: np.matrix , kFrequent: list[float]) -> float:\n",
    "        support = 0\n",
    "        for j,subset in enumerate(set):\n",
    "            mask = np.isin(subset,kFrequent)\n",
    "            if(np.sum(mask)==len(kFrequent)):\n",
    "                    support += 1\n",
    "        return support\n",
    "\n",
    "    \"\"\"\n",
    "    X->Y is stored as followed\n",
    "    [[[X1],[Y1]],\n",
    "     [[X2],[Y2]],\n",
    "        ...\n",
    "     [[Xn],[Yn]]]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def findSimpleAssociationRules(self, set : np.matrix ,confidence : float) -> np.matrix:\n",
    "        simpleAssociationRules = []\n",
    "        for kFrequents in self.itemFound[1:]: #no need to investigate singletons\n",
    "            for i,kFrequent in enumerate(kFrequents):\n",
    "                supportN = self.computeSupport(set,kFrequent)\n",
    "                for j in range(len(kFrequent)-1,-1,-1):\n",
    "                    temp = np.delete(kFrequent,j)\n",
    "                    #print(temp)\n",
    "                    supportD = self.computeSupport(set,temp)\n",
    "                    if(supportN/supportD >= confidence):\n",
    "                        simpleAssociationRules.append([temp,[kFrequent[j]]])\n",
    "                        #print([temp,[kFrequent[j]]])\n",
    "                        self.mineAssociationRules(set,temp,[kFrequent[j]],simpleAssociationRules,supportN,confidence)\n",
    "                        #print(f\"building association rules = {simpleAssociationRules}\")\n",
    "        return simpleAssociationRules\n",
    "                    \n",
    "    def mineAssociationRules(self,set: np.matrix , X : list[float], Y : list[float], rules : list[any], supportN : int, confidence : float)-> np.matrix:\n",
    "        if(len(X)<=1):\n",
    "            return rules\n",
    "        else:\n",
    "            for i in range(len(X)):\n",
    "                temp = np.delete(X,i)\n",
    "                #print(Y)\n",
    "                supportD = self.computeSupport(set,temp)\n",
    "                if(supportN/supportD >= confidence):\n",
    "                    rules.append([temp,Y+[X[i]]])\n",
    "                    #print([temp,Y+[X[i]]])\n",
    "                    self.mineAssociationRules(set,temp,Y+[X[i]],rules,supportN,confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c64e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix =[]\n",
    "\n",
    "with open(\"T10I4D100K.dat\", \"r\") as f:\n",
    "    for line in f:\n",
    "        row = list(map(int, line.split()))\n",
    "        matrix.append(row)\n",
    "\n",
    "timekFrequent = []\n",
    "timeAssociation = []\n",
    "itemFinder = FrequentItem()\n",
    "\n",
    "thresholds = [4,5,7,10]\n",
    "for i in thresholds:\n",
    "    print(f'finding frequent items t={i/100}')\n",
    "    e = t.time()\n",
    "    singletons = itemFinder.findSingletons(matrix,i/100)\n",
    "    kFrequents = itemFinder.getFrequentItemFinder(i/100,matrix,[singletons],singletons)\n",
    "    timekFrequent.append((t.time()-e)*1000)\n",
    "    print(f'finding associations rules')\n",
    "    e = t.time()\n",
    "    simpleAsso = itemFinder.findSimpleAssociationRules(matrix,0.5)\n",
    "    timeAssociation.append((t.time()-e)*1000)\n",
    "\n",
    "plt.scatter(thresholds,timekFrequent,label='kFrequent')\n",
    "plt.xlabel(\"threshold (%)\")\n",
    "plt.ylabel(\"time in ms\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(thresholds,timeAssociation,label='Association')\n",
    "plt.xlabel(\"threshold (%)\")\n",
    "plt.ylabel(\"time in ms\")\n",
    "plt.show()\n",
    "\n",
    "timeAssociation = []\n",
    "\n",
    "confidence = [40,50,70,90]\n",
    "singletons = itemFinder.findSingletons(matrix,0.03)\n",
    "kFrequents = itemFinder.getFrequentItemFinder(0.03,matrix,[singletons],singletons)\n",
    "\n",
    "for i in confidence:\n",
    "    print(f'finding associations rules c={i/100}')\n",
    "    e = t.time()\n",
    "    simpleAsso = itemFinder.findSimpleAssociationRules(matrix,i/100)\n",
    "    timeAssociation.append((t.time()-e)*1000)\n",
    "\n",
    "plt.scatter(confidence,timeAssociation,label='Association')\n",
    "plt.xlabel(\"confidence (%)\")\n",
    "plt.ylabel(\"time in ms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea2188",
   "metadata": {},
   "source": [
    "All datasets used throughout this lab are publicly available in the following GitHub repository: https://github.com/maximesf/lab_ID2222/ To access them, navigate to the lab1/data/ directory within the repository, where all the files used in our experiments are stored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
