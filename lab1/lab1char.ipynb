{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Finding Similar Items: Textually Similar Documents\n",
    "\n",
    "This notebook implements a pipeline to compute textual similarity between documents using:\n",
    "\n",
    "- k-Shingles\n",
    "- Jaccard similarity\n",
    "- MinHash signatures\n",
    "- Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "Dataset: BBC news articles (text files)\n",
    "https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib as h\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shingling Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "The Shingling class is responsible for creating k-shingles from a document. Shingles can be either sequences of characters or words, depending on the boolean attribute word.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "Removes special characters defined in removeCaracter to standardize text\n",
    "\n",
    "Uses the md5 function from Python’s hashlib module to hash the shingles, which are required for the MinHashing step.\n",
    "\n",
    "Supports both text (.txt) and CSV (.csv) files.\n",
    "\n",
    "Provides methods to generate raw k-shingles, hashed shingles, and unique hashed shingles for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shingling:\n",
    "    k: int\n",
    "    word: bool\n",
    "    shingle: list[str]\n",
    "    hashShingle: list[str]\n",
    "    removeCaracter: list[str]\n",
    "\n",
    "    def __init__(self, k: int = 1, word = False) -> None:\n",
    "        self.k = k\n",
    "        self.word = word\n",
    "        self.removeCaracter = '()+=-_!,;.:/?'\n",
    "    \n",
    "    def _read_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Lecture du fichier .csv ou .txt\n",
    "        Retourne tout le texte en minuscule.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        # Lecture CSV\n",
    "        if ext == \".csv\":\n",
    "            with open(file_path, newline='', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                text_list = []\n",
    "                for row in reader:\n",
    "                    if row and row[0].strip():\n",
    "                        text_list.append(row[0].strip())\n",
    "                return \" \".join(text_list).lower()\n",
    "\n",
    "        # Lecture TXT\n",
    "        elif ext == \".txt\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read().lower()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Format de fichier non pris en charge : {file_path}\")\n",
    "        \n",
    "\n",
    "    def kShingling(self, file_path: str) -> list[str]:\n",
    "        text = self._read_file(file_path)\n",
    "        kShingle = []\n",
    "\n",
    "        #text = text.lower()\n",
    "        \n",
    "        for i in range(len(self.removeCaracter)):\n",
    "            text = text.replace(self.removeCaracter[i],\"\")\n",
    "\n",
    "        if(self.word):\n",
    "            wordList = text.split(\" \")\n",
    "            for i in range(len(wordList)):\n",
    "                if(i+self.k>len(wordList)):\n",
    "                    self.shingle = kShingle\n",
    "                    return kShingle\n",
    "                key = ''\n",
    "                for j in range(self.k):\n",
    "                    key += wordList[i+j] + ' '\n",
    "                key = key[:len(key)-1]\n",
    "                kShingle.append(key)\n",
    "        else:\n",
    "            for i in range(len(text)):\n",
    "                if(i+self.k>len(text)):\n",
    "                    self.shingle = kShingle\n",
    "                    return kShingle\n",
    "                key = ''\n",
    "                for j in range(self.k):\n",
    "                    key += text[i+j]\n",
    "                kShingle.append(key)\n",
    "        self.shingle = kShingle\n",
    "        return kShingle\n",
    "\n",
    "    \n",
    "\n",
    "    def hashShingling(self, file_path: str) -> list[str]:\n",
    "        hashShingle = []\n",
    "        kShingle = self.kShingling(file_path)\n",
    "        for word in kShingle:\n",
    "            hashShingle.append(h.md5(word.encode()))\n",
    "        self.hashShingle = hashShingle\n",
    "        return hashShingle\n",
    "    \n",
    "    def uniqueHashShingling(self, file_path: str) -> list[int]:\n",
    "        hashShingle = []\n",
    "        kShingle = self.kShingling(file_path)\n",
    "        kShingle = np.unique(kShingle)\n",
    "        for word in kShingle:\n",
    "            hashShingle.append(h.md5(word.encode()).hexdigest())\n",
    "        self.hashShingle = hashShingle\n",
    "        return hashShingle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CompareSets**\n",
    "The CompareSets class computes the exact Jaccard similarity between two sets of hashed shingles.\n",
    "Key Method:\n",
    "\n",
    "getJaccardSim(set1, set2): Computes the ratio of the intersection over the union of two sets.\n",
    "\n",
    "Use:\n",
    "This provides the ground truth similarity for documents, which we can compare with the approximate similarity estimated by MinHashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSets:\n",
    "    def __init__(self) -> None:\n",
    "        print('Compare Sets class')\n",
    "        \n",
    "    def getJaccardSim(self,set1,set2) -> float:\n",
    "        inter = 0\n",
    "        union = 1\n",
    "        for hash1 in set1:\n",
    "            for hash2 in set2:\n",
    "                if(hash1==hash2):\n",
    "                    inter += 1\n",
    "        union = len(np.unique(set1 + set2))\n",
    "        return inter/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minhash**\n",
    "\n",
    "-Purpose:\n",
    "The MinHashing class builds a MinHash signature for a document from its set of shingles. This signature is a vector of integers that approximates the Jaccard similarity between documents.\n",
    "\n",
    "-Features:\n",
    "\n",
    "Uses multiple hash functions of the form:\n",
    "hi​(x)=(ai​⋅x+bi​)%c\n",
    "\n",
    "a and b are random coefficients\n",
    "c is a large prime.\n",
    "\n",
    "-Result:\n",
    "MinHash allows us to estimate similarity much faster than computing exact Jaccard, especially for large document sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minhash version with permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class MinHashing:\n",
    "#     def __init__(self) -> None:\n",
    "#         print('MinHashing class')\n",
    "        \n",
    "#     def buildSignature(self,set1,set2,nbPermut) -> float:\n",
    "        \n",
    "#         union = np.unique(set1 + set2)\n",
    "#         mask1 = np.isin(union, set1)\n",
    "#         mask2 = np.isin(union, set2)\n",
    "        \n",
    "#         signatureMatrix = np.empty((0, 2), int)\n",
    "#         for _ in range(nbPermut):\n",
    "#             permutedIndexes = np.random.permutation(len(union))\n",
    "#             lineSignatureMatrix = np.zeros((1,2))\n",
    "#             for i in range(len(permutedIndexes)):\n",
    "#                 if(mask1[permutedIndexes[i]]):\n",
    "#                     lineSignatureMatrix[0][0]=i\n",
    "#                     break\n",
    "#             for j in range(len(permutedIndexes)):\n",
    "#                 if(mask2[permutedIndexes[j]]):\n",
    "#                     lineSignatureMatrix[0][1]=j\n",
    "#                     break\n",
    "#             signatureMatrix = np.vstack((signatureMatrix, lineSignatureMatrix))\n",
    "\n",
    "#         return signatureMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinHashing (Using Hashing functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MinHashing:\n",
    "    def __init__(self, num_hashes=100, max_shingle=2**32 - 1, seed=42):\n",
    "        self.num_hashes = num_hashes\n",
    "        self.max_shingle = max_shingle\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Calculating the coeffs for the hashing fct: a and b, h_i(x) = (a_i * x + b_i) % c\n",
    "        self.a = [random.randint(1, max_shingle - 1) for _ in range(num_hashes)]\n",
    "        self.b = [random.randint(0, max_shingle - 1) for _ in range(num_hashes)]\n",
    "        self.c = 4294967311 # prime number + must be > max(shingle) , shingles are converted into integers after the MD5\n",
    "\n",
    "    def compute_signature(self, shingle_set):\n",
    "        \"\"\"\n",
    "        Signature for one doc returns a vector of integers\n",
    "        \"\"\"\n",
    "        sig = np.full(self.num_hashes, np.inf)\n",
    "\n",
    "        for shingle in shingle_set:\n",
    "            for i in range(self.num_hashes):\n",
    "                h_val = (self.a[i] * shingle + self.b[i]) % self.c\n",
    "                if h_val < sig[i]:\n",
    "                    sig[i] = h_val\n",
    "\n",
    "        return sig.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CompareSignatures**\n",
    "\n",
    "Purpose:\n",
    "The CompareSignatures class computes the similarity between two MinHash signatures.\n",
    "\n",
    "Key Method:\n",
    "\n",
    "computeEstimateSimilarity(signature_matrix): Returns the fraction of positions where the two signatures are identical.\n",
    "\n",
    "Use:\n",
    "This gives an approximate similarity between documents without using the full sets of shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSignatures:\n",
    "    def __init__(self) -> None:\n",
    "        print('Compare Signatures class')\n",
    "        \n",
    "    def computeEstimateSimilarity(self,signature) -> float:\n",
    "        \n",
    "        signature = signature.T\n",
    "\n",
    "        y = signature[0]-signature[1]\n",
    "        y = np.where(y==0,1,0)\n",
    "        \n",
    "        return np.sum(y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LocalSensitiveHash**\n",
    "\n",
    "Purpose:\n",
    "\n",
    "The LocalSensitiveHash class implements the Locality Sensitive Hashing (LSH) technique to efficiently find candidate pairs of similar documents. Instead of comparing every pair of documents (which can be very slow for large datasets), LSH reduces the number of comparisons by focusing only on likely similar pairs.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Banding:\n",
    "\n",
    "The MinHash signature of each document is divided into band bands.\n",
    "\n",
    "Each band contains r consecutive rows (elements) from the signature.\n",
    "\n",
    "This splits the signature matrix into smaller pieces that can be hashed separately.\n",
    "\n",
    "Candidate Detection:\n",
    "\n",
    "For a given pair of documents, the method compares each corresponding band.\n",
    "\n",
    "If the hash of a band matches for the two documents, that band contributes to their similarity score.\n",
    "\n",
    "A pair is considered a candidate if the fraction of matching bands is above the threshold.\n",
    "\n",
    "Hashing Bands:\n",
    "\n",
    "Each band is converted to a string and hashed using md5.\n",
    "\n",
    "Matching hashes indicate that the band is identical for both documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LocalSensitiveHash:\n",
    "#     band: int\n",
    "#     r: int\n",
    "#     threshold: int\n",
    "\n",
    "#     def __init__(self, band : int, threshold : int) -> None:\n",
    "#         self.band = band\n",
    "#         self.threshold = threshold\n",
    "\n",
    "#     def lookForCandidates(self,signature,col1 : int = 0,col2 : int = 1) -> float:\n",
    "        \n",
    "#         self.r = int(len(signature)/self.band)\n",
    "#         similarities = 0\n",
    "#         if(self.r * self.band == len(signature)):\n",
    "#             signature = signature.T #transpose the signature matrix so that each column corresponds to a document.\n",
    "#             for i in range(self.band):\n",
    "#                 band1 = str(signature[col1][i:i+self.r])\n",
    "#                 band2 = str(signature[col2][i:i+self.r])\n",
    "                \n",
    "#                 if(h.md5(band1.encode()).hexdigest()==h.md5(band2.encode()).hexdigest()):\n",
    "#                     similarities += 1\n",
    "#                     print(\"similarity found\")\n",
    "#             print(f\"similarities = {similarities/self.band}\")\n",
    "#             if(similarities/self.band >= self.threshold):\n",
    "#                 return 1\n",
    "#             else:\n",
    "#                 return 0\n",
    "            \n",
    "#         else:\n",
    "#             print(\"Wrong size\")\n",
    "#             return -1\n",
    "import hashlib as h\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "class LocalSensitiveHashMulti:\n",
    "    \"\"\"\n",
    "    LSH for many documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, band: int = 20, threshold: float = 0.7):\n",
    "        self.band = band\n",
    "        self.threshold = threshold\n",
    "        self.r = None  # rows per band\n",
    "\n",
    "    def find_candidates(self, signatures: dict):\n",
    "        \"\"\"\n",
    "        signatures: dict {doc_name: signature_vector }\n",
    "        Returns a set of candidate pairs (doc1, doc2)\n",
    "        \"\"\"\n",
    "        # Convert signatures dict to matrix form for convenience\n",
    "        doc_names = list(signatures.keys())\n",
    "        sig_matrix = np.array([signatures[doc] for doc in doc_names])\n",
    "        sig_matrix = sig_matrix.T  # rows = hash values\n",
    "                                    #columns = documents\n",
    "\n",
    "        self.r = sig_matrix.shape[0] // self.band\n",
    "        if self.r * self.band != sig_matrix.shape[0]:\n",
    "            raise ValueError(\"Number of hash functions must be divisible by number of bands\")\n",
    "\n",
    "        candidates = set()\n",
    "\n",
    "        for b in range(self.band):\n",
    "            band_hash_table = defaultdict(list)\n",
    "            start = b * self.r\n",
    "            end = (b + 1) * self.r\n",
    "\n",
    "            # Hash each document's band\n",
    "            for col, doc in enumerate(doc_names):\n",
    "                band = sig_matrix[start:end, col]\n",
    "                band_str = \",\".join(map(str, band))\n",
    "                band_hash = h.md5(band_str.encode()).hexdigest()\n",
    "                band_hash_table[band_hash].append(doc)\n",
    "\n",
    "            # Any documents sharing the same band hash are candidate pairs\n",
    "            for bucket_docs in band_hash_table.values():\n",
    "                if len(bucket_docs) > 1:\n",
    "                    for doc1, doc2 in combinations(bucket_docs, 2):\n",
    "                        candidates.add(tuple(sorted([doc1, doc2])))\n",
    "\n",
    "        return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First test with 2 simple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSHasher = LocalSensitiveHash(25,0.8) \n",
    "# estimator = CompareSignatures()\n",
    "# miniHasher = MinHashing()\n",
    "# comparator = CompareSets()\n",
    "\n",
    "# fiveShingler = Shingling(3,False)\n",
    "\n",
    "# #kShingle = fiveShingler.kShingling('test.csv')\n",
    "# #hashList = fiveShingler.uniqueHashShingling('test.csv')\n",
    "# #print(f\"len = {len(hashList)}\")\n",
    "# hashShingle = fiveShingler.uniqueHashShingling('1.csv')\n",
    "# test3 = fiveShingler.uniqueHashShingling('2.csv')\n",
    "# #print(kShingle)\n",
    "# #print(hashShingle)\n",
    "\n",
    "# similarity = comparator.getJaccardSim(hashShingle,test3)\n",
    "# signature = miniHasher.buildSignature(hashShingle,test3,nbPermut=10000)\n",
    "# estimate = estimator.computeEstimateSimilarity(signature)\n",
    "# #print(f\"longueur de la signature {len(signature)}\")\n",
    "# retour = LSHasher.lookForCandidates(signature)\n",
    "# print(f\"r = {LSHasher.r}, retour de LSH {retour}\")\n",
    "\n",
    "# #print(signature)\n",
    "# print(similarity)\n",
    "# print(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- TEST EXAMPLE -------------------\n",
    "In this test, we create two very similar short documents.Both sentences describe \"the quick brown fox jumping over the lazy dog\",with the second document containing just one additional phrase (\"et joue\").This allows us to verify whether our pipeline (Shingling → MinHash → LSH)correctly detects similarity between documents that only differ slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Sets class\n",
      "Compare Signatures class\n",
      "\n",
      "Comparing doc1.txt <-> doc2.txt\n",
      "Candidate (LSH): {('doc1.txt', 'doc2.txt')}\n",
      "Jaccard exact: 0.8621\n",
      "Estimated MinHash: 0.8600\n"
     ]
    }
   ],
   "source": [
    "docs = [\"doc1.txt\", \"doc2.txt\"]\n",
    "texts = [\n",
    "    \"le rapide renard brun saute par dessus le chien paresseux\",\n",
    "    \"le rapide renard brun saute par dessus le chien paresseux et joue\"\n",
    "]\n",
    "for name, txt in zip(docs, texts):\n",
    "    with open(name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "\n",
    "# ------------------- Classes -------------------\n",
    "Shinglers = Shingling(k=3, word=False)\n",
    "comparator = CompareSets()\n",
    "miniHasher = MinHashing(num_hashes=100)\n",
    "estimator = CompareSignatures()\n",
    "LSHasher = LocalSensitiveHashMulti(band=25, threshold=0.6)\n",
    "\n",
    "# ------------------- Shingles and signatures -------------------\n",
    "shingles_dict = {}\n",
    "signatures = {}\n",
    "for doc in docs:\n",
    "    shingles = Shinglers.uniqueHashShingling(doc)\n",
    "    shingles_int = [int(x,16) for x in shingles]\n",
    "    shingles_dict[doc] = shingles_int\n",
    "    signatures[doc] = miniHasher.compute_signature(shingles_int)\n",
    "\n",
    "# ------------------- Comparing -------------------\n",
    "sig_matrix = np.array([signatures[docs[0]], signatures[docs[1]]]).T\n",
    "candidate = LSHasher.find_candidates(signatures)\n",
    "exact = comparator.getJaccardSim(shingles_dict[docs[0]], shingles_dict[docs[1]])\n",
    "estimate = estimator.computeEstimateSimilarity(sig_matrix)\n",
    "\n",
    "print(f\"\\nComparing {docs[0]} <-> {docs[1]}\")\n",
    "print(f\"Candidate (LSH): {candidate}\")\n",
    "print(f\"Jaccard exact: {exact:.4f}\")\n",
    "print(f\"Estimated MinHash: {estimate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------Testing with many docs ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Dataset** (Business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Files to compare ---\n",
    "#docs = [\"1.csv\", \"2.csv\", \"3.csv\",\"4.csv\",\"5.csv\"]\n",
    "folder_path = \"data/business\"  \n",
    "\n",
    "# --- Extracting all files ---\n",
    "docs = [\n",
    "    os.path.join(folder_path, f)\n",
    "    for f in os.listdir(folder_path)\n",
    "    if f.endswith(\".txt\") or f.endswith(\".csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Sets class\n",
      "Compare Signatures class\n",
      "LSH candidate pair: data/business\\001.txt <-> data/business\\015.txt\n",
      "LSH candidate pair: data/business\\002.txt <-> data/business\\031.txt\n",
      "LSH candidate pair: data/business\\002.txt <-> data/business\\053.txt\n",
      "LSH candidate pair: data/business\\002.txt <-> data/business\\087.txt\n",
      "LSH candidate pair: data/business\\003.txt <-> data/business\\060.txt\n",
      "LSH candidate pair: data/business\\004.txt <-> data/business\\027.txt\n",
      "LSH candidate pair: data/business\\004.txt <-> data/business\\030.txt\n",
      "LSH candidate pair: data/business\\004.txt <-> data/business\\077.txt\n",
      "LSH candidate pair: data/business\\004.txt <-> data/business\\090.txt\n",
      "LSH candidate pair: data/business\\005.txt <-> data/business\\047.txt\n",
      "LSH candidate pair: data/business\\005.txt <-> data/business\\067.txt\n",
      "LSH candidate pair: data/business\\005.txt <-> data/business\\082.txt\n",
      "LSH candidate pair: data/business\\006.txt <-> data/business\\049.txt\n",
      "LSH candidate pair: data/business\\007.txt <-> data/business\\019.txt\n",
      "LSH candidate pair: data/business\\007.txt <-> data/business\\022.txt\n",
      "LSH candidate pair: data/business\\008.txt <-> data/business\\009.txt\n",
      "LSH candidate pair: data/business\\008.txt <-> data/business\\018.txt\n",
      "LSH candidate pair: data/business\\008.txt <-> data/business\\029.txt\n",
      "LSH candidate pair: data/business\\008.txt <-> data/business\\033.txt\n",
      "LSH candidate pair: data/business\\008.txt <-> data/business\\052.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\018.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\029.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\033.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\039.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\044.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\047.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\068.txt\n",
      "LSH candidate pair: data/business\\009.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\010.txt <-> data/business\\029.txt\n",
      "LSH candidate pair: data/business\\010.txt <-> data/business\\031.txt\n",
      "LSH candidate pair: data/business\\013.txt <-> data/business\\023.txt\n",
      "LSH candidate pair: data/business\\013.txt <-> data/business\\041.txt\n",
      "LSH candidate pair: data/business\\013.txt <-> data/business\\064.txt\n",
      "LSH candidate pair: data/business\\014.txt <-> data/business\\091.txt\n",
      "LSH candidate pair: data/business\\014.txt <-> data/business\\093.txt\n",
      "LSH candidate pair: data/business\\015.txt <-> data/business\\052.txt\n",
      "LSH candidate pair: data/business\\015.txt <-> data/business\\083.txt\n",
      "LSH candidate pair: data/business\\018.txt <-> data/business\\029.txt\n",
      "LSH candidate pair: data/business\\018.txt <-> data/business\\033.txt\n",
      "LSH candidate pair: data/business\\020.txt <-> data/business\\057.txt\n",
      "LSH candidate pair: data/business\\020.txt <-> data/business\\097.txt\n",
      "LSH candidate pair: data/business\\022.txt <-> data/business\\041.txt\n",
      "LSH candidate pair: data/business\\022.txt <-> data/business\\045.txt\n",
      "LSH candidate pair: data/business\\022.txt <-> data/business\\050.txt\n",
      "LSH candidate pair: data/business\\022.txt <-> data/business\\065.txt\n",
      "LSH candidate pair: data/business\\022.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\024.txt <-> data/business\\081.txt\n",
      "LSH candidate pair: data/business\\025.txt <-> data/business\\029.txt\n",
      "LSH candidate pair: data/business\\025.txt <-> data/business\\057.txt\n",
      "LSH candidate pair: data/business\\025.txt <-> data/business\\083.txt\n",
      "LSH candidate pair: data/business\\025.txt <-> data/business\\100.txt\n",
      "LSH candidate pair: data/business\\027.txt <-> data/business\\077.txt\n",
      "LSH candidate pair: data/business\\028.txt <-> data/business\\040.txt\n",
      "LSH candidate pair: data/business\\028.txt <-> data/business\\047.txt\n",
      "LSH candidate pair: data/business\\028.txt <-> data/business\\061.txt\n",
      "LSH candidate pair: data/business\\029.txt <-> data/business\\031.txt\n",
      "LSH candidate pair: data/business\\029.txt <-> data/business\\033.txt\n",
      "LSH candidate pair: data/business\\029.txt <-> data/business\\057.txt\n",
      "LSH candidate pair: data/business\\031.txt <-> data/business\\052.txt\n",
      "LSH candidate pair: data/business\\031.txt <-> data/business\\053.txt\n",
      "LSH candidate pair: data/business\\031.txt <-> data/business\\055.txt\n",
      "LSH candidate pair: data/business\\031.txt <-> data/business\\087.txt\n",
      "LSH candidate pair: data/business\\033.txt <-> data/business\\072.txt\n",
      "LSH candidate pair: data/business\\034.txt <-> data/business\\094.txt\n",
      "LSH candidate pair: data/business\\034.txt <-> data/business\\097.txt\n",
      "LSH candidate pair: data/business\\038.txt <-> data/business\\049.txt\n",
      "LSH candidate pair: data/business\\040.txt <-> data/business\\047.txt\n",
      "LSH candidate pair: data/business\\040.txt <-> data/business\\061.txt\n",
      "LSH candidate pair: data/business\\040.txt <-> data/business\\073.txt\n",
      "LSH candidate pair: data/business\\041.txt <-> data/business\\045.txt\n",
      "LSH candidate pair: data/business\\041.txt <-> data/business\\050.txt\n",
      "LSH candidate pair: data/business\\041.txt <-> data/business\\065.txt\n",
      "LSH candidate pair: data/business\\041.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\045.txt <-> data/business\\050.txt\n",
      "LSH candidate pair: data/business\\045.txt <-> data/business\\065.txt\n",
      "LSH candidate pair: data/business\\045.txt <-> data/business\\080.txt\n",
      "LSH candidate pair: data/business\\045.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\046.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\047.txt <-> data/business\\061.txt\n",
      "LSH candidate pair: data/business\\048.txt <-> data/business\\051.txt\n",
      "LSH candidate pair: data/business\\048.txt <-> data/business\\092.txt\n",
      "LSH candidate pair: data/business\\049.txt <-> data/business\\079.txt\n",
      "LSH candidate pair: data/business\\050.txt <-> data/business\\065.txt\n",
      "LSH candidate pair: data/business\\050.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\052.txt <-> data/business\\080.txt\n",
      "LSH candidate pair: data/business\\053.txt <-> data/business\\087.txt\n",
      "LSH candidate pair: data/business\\056.txt <-> data/business\\076.txt\n",
      "LSH candidate pair: data/business\\063.txt <-> data/business\\072.txt\n",
      "LSH candidate pair: data/business\\063.txt <-> data/business\\090.txt\n",
      "LSH candidate pair: data/business\\065.txt <-> data/business\\090.txt\n",
      "LSH candidate pair: data/business\\065.txt <-> data/business\\092.txt\n",
      "LSH candidate pair: data/business\\065.txt <-> data/business\\099.txt\n",
      "LSH candidate pair: data/business\\066.txt <-> data/business\\073.txt\n",
      "LSH candidate pair: data/business\\066.txt <-> data/business\\089.txt\n",
      "LSH candidate pair: data/business\\067.txt <-> data/business\\082.txt\n",
      "LSH candidate pair: data/business\\075.txt <-> data/business\\097.txt\n",
      "LSH candidate pair: data/business\\079.txt <-> data/business\\098.txt\n",
      "LSH candidate pair: data/business\\080.txt <-> data/business\\093.txt\n",
      "LSH candidate pair: data/business\\081.txt <-> data/business\\084.txt\n",
      "LSH candidate pair: data/business\\081.txt <-> data/business\\098.txt\n",
      "LSH candidate pair: data/business\\083.txt <-> data/business\\100.txt\n",
      "LSH candidate pair: data/business\\084.txt <-> data/business\\098.txt\n",
      "LSH candidate pair: data/business\\087.txt <-> data/business\\100.txt\n",
      "LSH candidate pair: data/business\\090.txt <-> data/business\\092.txt\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Classes -------------------\n",
    "Shinglers = Shingling(k=3, word=False)\n",
    "comparator = CompareSets()\n",
    "miniHasher = MinHashing(num_hashes=100)\n",
    "estimator = CompareSignatures()\n",
    "LSHasher = LocalSensitiveHashMulti(band=20, threshold=0.7)\n",
    "\n",
    "# ------------------- Shingles and signatures -------------------\n",
    "shingles_dict = {}\n",
    "signatures = {}\n",
    "for doc in docs:\n",
    "    shingles = Shinglers.uniqueHashShingling(doc)\n",
    "    shingles_int = [int(x,16) for x in shingles]\n",
    "    shingles_dict[doc] = shingles_int\n",
    "    signatures[doc] = miniHasher.compute_signature(shingles_int)\n",
    "\n",
    "# ------------------- Comparing -------------------\n",
    "import itertools\n",
    "import numpy as np\n",
    "# Open a CSV file to write the results\n",
    "# --- Find all candidate pairs using LSHMulti ---\n",
    "candidates = LSHasher.find_candidates(signatures)  # signatures = dict {doc_name: signature}\n",
    "\n",
    "# Open CSV to write results\n",
    "with open(\"lsh_results.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Document 1\", \"Document 2\", \"LSH Candidate\", \"Jaccard Exact\", \"MinHash Estimate\"])\n",
    "\n",
    "    for doc1, doc2 in itertools.combinations(docs, 2):\n",
    "        sig_matrix = np.array([signatures[doc1], signatures[doc2]]).T\n",
    "\n",
    "        # Check if this pair is an LSH candidate\n",
    "        is_candidate = 1 if (doc1, doc2) in candidates else 0\n",
    "        if is_candidate:\n",
    "            print(f\"LSH candidate pair: {doc1} <-> {doc2}\")\n",
    "\n",
    "        # Exact Jaccard\n",
    "        exact = comparator.getJaccardSim(shingles_dict[doc1], shingles_dict[doc2])\n",
    "\n",
    "        # Estimated MinHash similarity\n",
    "        estimate = estimator.computeEstimateSimilarity(sig_matrix)\n",
    "\n",
    "        # Write row\n",
    "        writer.writerow([doc1, doc2, is_candidate, f\"{exact:.4f}\", f\"{estimate:.4f}\"])\n",
    "\n",
    "# with open(\"lsh_results.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "    \n",
    "#     # Write the header\n",
    "#     writer.writerow([\"Document 1\", \"Document 2\", \"LSH Candidate\", \"Jaccard Exact\", \"MinHash Estimate\"])\n",
    "#     for doc1, doc2 in itertools.combinations(docs, 2):\n",
    "#         sig_matrix = np.array([signatures[doc1], signatures[doc2]]).T\n",
    "        \n",
    "#         # LSH\n",
    "#         candidate = LSHasher.find_candidates(signatures)\n",
    "#             # printing directly the matching candidate\n",
    "#         if candidate == 1:\n",
    "#             print(f\"LSH candidate pair: {doc1} <-> {doc2}\")\n",
    "        \n",
    "#         # Jaccard exact\n",
    "#         exact = comparator.getJaccardSim(shingles_dict[doc1], shingles_dict[doc2])\n",
    "        \n",
    "#         # Estimation MinHash\n",
    "#         estimate = estimator.computeEstimateSimilarity(sig_matrix)\n",
    "#                 # Write the row\n",
    "#         writer.writerow([doc1, doc2, candidate, f\"{exact:.4f}\", f\"{estimate:.4f}\"])\n",
    "    \n",
    "#     print(f\"\\nComparing {doc1} <-> {doc2}\")\n",
    "#     print(f\"Candidate (LSH): {candidate}\")\n",
    "#     print(f\"Jaccard exact: {exact:.4f}\")\n",
    "#     print(f\"Estimated MinHash: {estimate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some plots/comparing results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------Visualising Jaccard Similarity-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we visualize the exact Jaccard similarities between all document pairs.\n",
    "This helps us understand the overall similarity structure within the dataset\n",
    "and validate whether similar documents are indeed detected as such.\n",
    "For the dataset, we used 5 mails having some similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# N = len(docs)\n",
    "# jaccard_matrix = np.zeros((N, N))\n",
    "# Compute pairwise Jaccard similarity between all document pairs\n",
    "# for i, doc1 in enumerate(docs):\n",
    "#     for j, doc2 in enumerate(docs):\n",
    "#         jaccard_matrix[i, j] = comparator.getJaccardSim(shingles_dict[doc1], shingles_dict[doc2])\n",
    "# We use Seaborn to create a heatmap showing similarity values between all documents.\n",
    "# The heatmap provides a quick visual overview of which documents are most similar.\n",
    "# sns.heatmap(jaccard_matrix, annot=True, xticklabels=docs, yticklabels=docs, cmap=\"viridis\")\n",
    "# plt.title(\"Exact Jaccard Similarity Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  VISUALIZING MINHASH ESTIMATED SIMILARITIES \n",
    "This section computes and visualizes the *estimated* document similarities obtained using the MinHash signatures (instead of the exact Jaccard similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minhash_matrix = np.zeros((N, N))\n",
    "\n",
    "# for i, doc1 in enumerate(docs):\n",
    "#     for j, doc2 in enumerate(docs):\n",
    "#         # matrice des signatures pour le computeEstimateSimilarity\n",
    "#         sig_matrix = np.array([signatures[doc1], signatures[doc2]]).T\n",
    "#         minhash_matrix[i, j] = estimator.computeEstimateSimilarity(sig_matrix)\n",
    "\n",
    "# # --- Heatmap ---\n",
    "# Similar to the previous visualization, we now display the estimated similarities\n",
    "# in the form of a heatmap using Seaborn.\n",
    "# sns.heatmap(minhash_matrix, annot=True, xticklabels=docs, yticklabels=docs, cmap=\"viridis\")\n",
    "# plt.title(\"Estimated MinHash Similarity Matrix\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
