{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ne2eFxFvUKe",
        "outputId": "fca47247-42cb-435b-d59f-843bdca1a778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrF38OYNMGue",
        "outputId": "6d71a21f-57d5-49d8-a3e4-9aa390c5962e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mmh3 in /usr/local/lib/python3.12/dist-packages (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mmh3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lw--GsQv0rp",
        "outputId": "dbd0ed35-b0d5-403d-a938-a23d23d98f7e"
      },
      "outputs": [],
      "source": [
        "# !apt-get update\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Tead4yqX1Zao"
      },
      "outputs": [],
      "source": [
        "# import pyspark\n",
        "# from pyspark.sql import DataFrame, SparkSession\n",
        "# from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "# from pyspark.sql.functions import col\n",
        "# import hashlib\n",
        "# from pyspark.sql.functions import lower, regexp_replace, split, udf\n",
        "# from pyspark.sql.types import ArrayType, StringType, LongType, IntegerType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlT_PRoKlGE"
      },
      "source": [
        "Initialiser Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V_uIq55J1tN0"
      },
      "outputs": [],
      "source": [
        "# spark = SparkSession.builder \\\n",
        "#     .master(\"local[*]\") \\\n",
        "#     .appName(\"Document Similarity\") \\\n",
        "#     .getOrCreate()\n",
        "# sc = spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HCxKUSA7L_Sb"
      },
      "outputs": [],
      "source": [
        "import mmh3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQzWPUL9UuHU"
      },
      "source": [
        "**Classe Shingling(Spark)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GwqjFNnnveeh"
      },
      "outputs": [],
      "source": [
        "# # class Shingling():\n",
        "# #     def __init__(self, k=10):\n",
        "# #         \"\"\"\n",
        "# #         k: length of shingles (in words)\n",
        "# #         \"\"\"\n",
        "# #         self.k = k\n",
        "\n",
        "# #     def clean_text(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "# #         # Mettre en minuscule, supprimer caractères spéciaux, réduire espaces multiples\n",
        "# #         df = df.withColumn(text_col, lower(col(text_col)))\n",
        "# #         df = df.withColumn(text_col, regexp_replace(col(text_col), r\"[^a-z0-9\\s]\", \"\"))\n",
        "# #         df = df.withColumn(text_col, regexp_replace(col(text_col), r\"\\s+\", \" \"))\n",
        "# #         return df\n",
        "\n",
        "# #     def tokenize(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "# #         df = df.withColumn(\"tokens\", split(col(text_col), \"\\\\s+\"))\n",
        "\n",
        "# #         def clean_tokens(tokens):\n",
        "# #             # Supprimer None et chaînes vides\n",
        "# #             return [str(t).strip() for t in tokens if t and str(t).strip()]\n",
        "\n",
        "# #         clean_udf = udf(clean_tokens, ArrayType(StringType())) #udf(...) transforme clean_tokens en fonction distribuée Spark.#Spark ne peut pas directement appliquer des fcts Python, besoin d'un wrapper -> udf\n",
        "# #         df = df.withColumn(\"tokens\", clean_udf(col(\"tokens\")))\n",
        "# #         return df\n",
        "\n",
        "# #     def generate_shingles(self, df: DataFrame) -> DataFrame:\n",
        "# #         k = self.k\n",
        "\n",
        "# #         def shingle_function(tokens):\n",
        "# #             tokens = [str(t).strip() for t in tokens if t and str(t).strip()]\n",
        "# #             if len(tokens) < k:\n",
        "# #                 return []\n",
        "# #             shingles = [\" \".join(tokens[i:i+k]) for i in range(len(tokens) - k + 1)]\n",
        "# #             # Supprimer shingles vides ou None\n",
        "# #             return [s for s in shingles if s and s.strip()]\n",
        "\n",
        "# #         shingle_udf = udf(shingle_function, ArrayType(StringType()))\n",
        "# #         df = df.withColumn(\"shingles\", shingle_udf(col(\"tokens\")))\n",
        "# #         return df\n",
        "\n",
        "# #     def hash_shingles(self, df: DataFrame) -> DataFrame:\n",
        "# #         def hash_list(shingles):\n",
        "# #             hashed = []\n",
        "# #             for s in shingles:\n",
        "# #                 if s and s.strip():\n",
        "# #                     h = mmh3.hash64(s.strip(), signed=False)[0] #retourne entier non signé entre 0 et 2^64 -1\n",
        "# #                     h = h % (2**63)  # forcer des entier entre 0 et 2^63 -1 #Sinon j'ai eu un overflow pour LongType et donc des valeurs NULL de hashage (mon prob initial)\n",
        "# #                     #h = mmh3.hash(\"some string\", signed=False)  # 32-bit unsigned integer\n",
        "# #                     hashed.append(h)\n",
        "# #             return hashed\n",
        "\n",
        "# #         hash_udf = udf(hash_list, ArrayType(LongType()))   #longtype entre - 2^63 et 2^63 -1\n",
        "# #         df = df.withColumn(\"hashed_shingles\", hash_udf(col(\"shingles\")))\n",
        "# #         return df\n",
        "\n",
        "# #     def transform(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "# #         df = self.clean_text(df, text_col)\n",
        "# #         df = self.tokenize(df, text_col)\n",
        "# #         df = self.generate_shingles(df)\n",
        "# #         df = self.hash_shingles(df)\n",
        "# #         return df\n",
        "\n",
        "# class Shingling():\n",
        "#     def __init__(self, k=10):\n",
        "#         self.k = k\n",
        "\n",
        "#     def transform(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "#         from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
        "#         from pyspark.sql.types import ArrayType, LongType, StringType\n",
        "#         import mmh3\n",
        "\n",
        "#         # Nettoyage du texte\n",
        "#         df = df.withColumn(text_col, lower(col(text_col)))\n",
        "#         df = df.withColumn(text_col, regexp_replace(col(text_col), r\"[^a-z0-9\\s]\", \"\"))\n",
        "#         df = df.withColumn(text_col, regexp_replace(col(text_col), r\"\\s+\", \" \"))\n",
        "\n",
        "#         # Tokenization\n",
        "#         df = df.withColumn(\"tokens\", split(col(text_col), \"\\\\s+\"))\n",
        "\n",
        "#         # Génération des shingles\n",
        "#         def shingles_fn(tokens):\n",
        "#             k = self.k\n",
        "#             tokens = [str(t).strip() for t in tokens if t and str(t).strip()]\n",
        "#             if len(tokens) < k:\n",
        "#                 return []\n",
        "#             return [\" \".join(tokens[i:i+k]) for i in range(len(tokens) - k + 1)]\n",
        "\n",
        "#         shingle_udf = udf(shingles_fn, ArrayType(StringType()))\n",
        "#         df = df.withColumn(\"shingles\", shingle_udf(col(\"tokens\")))\n",
        "\n",
        "#         # Hash des shingles\n",
        "#         def hash_fn(shingles):\n",
        "#             hashed = []\n",
        "#             for s in shingles:\n",
        "#                 if s and s.strip():\n",
        "#                     h = mmh3.hash64(s.strip(), signed=False)[0]\n",
        "#                     h = h % (2**63)\n",
        "#                     hashed.append(h)\n",
        "#             return hashed\n",
        "\n",
        "#         hash_udf = udf(hash_fn, ArrayType(LongType()))\n",
        "#         df = df.withColumn(\"hashed_shingles\", hash_udf(col(\"shingles\")))\n",
        "\n",
        "#         # Garder seulement filename et hashed_shingles\n",
        "#         df = df.select(\"filename\", \"hashed_shingles\")\n",
        "#         return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xtg81zzJyAQu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import mmh3\n",
        "\n",
        "class Shingling:\n",
        "    def __init__(self, k=10):\n",
        "        self.k = k  # nombre de mots par shingle\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        return text.split()\n",
        "\n",
        "    def generate_shingles(self, tokens):\n",
        "        k = self.k\n",
        "        return [\" \".join(tokens[i:i+k]) for i in range(len(tokens) - k + 1)]\n",
        "\n",
        "    def hash_shingles(self, shingles):\n",
        "        hashed = [mmh3.hash64(s, signed=False)[0] % (2**63) for s in shingles]\n",
        "        return hashed\n",
        "\n",
        "    def process_document(self, text: str):\n",
        "        cleaned = self.clean_text(text)\n",
        "        tokens = self.tokenize(cleaned)\n",
        "        shingles = self.generate_shingles(tokens)\n",
        "        hashed = self.hash_shingles(shingles)\n",
        "        return hashed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdSlp3JRVBCI"
      },
      "source": [
        "**Classe CompareSets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hiyA_Mi0veSb"
      },
      "outputs": [],
      "source": [
        "class CompareSets:\n",
        "    @staticmethod\n",
        "    def jaccard_similarity(set1, set2):\n",
        "        if not set1 or not set2:\n",
        "            return 0.0\n",
        "        s1, s2 = set(set1), set(set2)\n",
        "        return len(s1 & s2) / len(s1 | s2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5yxeue9Y-Oyi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class MinHashing():\n",
        "  def __init__(self,num_hashes = 100,max_hash = 2**32 -1):\n",
        "    self.num_hashes = num_hashes\n",
        "    self.max_hash = max_hash\n",
        "    self.hash_params = [(random.randint(1,self.max_hash -1),random.randint(0,self.max_hash-1)) for _ in range(num_hashes)]\n",
        "\n",
        "  def ComputeSignatures(self,shingle_set):\n",
        "    signature = []\n",
        "    for (a,b) in self.hash_params:\n",
        "      min_hash = min(((a*x + b)% self.max_hash) for x in shingle_set)\n",
        "      signature.append(min_hash)\n",
        "    return signature\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCeCusducrOc",
        "outputId": "3a84f550-8375-4835-f10d-89e63fcccb5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[825999843, 99439174, 1727863141, 141968942, 889074749, 3039606286, 2214678375, 1755671392, 52988319, 670821659, 582138174, 1493498556, 105392070, 2398914357, 711477627, 400207598, 2397516307, 595553760, 966339065, 2000690835, 1022071868, 1076582195, 2555345081, 1711784201, 307679545, 1314178635, 710824964, 3057683080, 48244578, 537393149, 1381948069, 228143124, 1307964398, 142082879, 818600845, 378778544, 626786850, 804932739, 61963376, 160258214, 1669587846, 2465295608, 306579501, 1260370449, 1769445955, 725765453, 590983094, 2374342158, 734578666, 111111468, 614692668, 889399249, 1592978693, 1415859935, 912682788, 1518048219, 1510305959, 103159389, 1960351194, 2671309449, 1017753898, 1112661943, 1771693509, 2068917019, 2328089292, 2123313183, 84293296, 1154031320, 2332659265, 79120369, 2033298130, 1656704445, 49766138, 1515914670, 451066912, 2121860713, 1185588738, 2256632871, 1948910719, 1710909440, 1307099233, 446893784, 1840000704, 893773593, 141276491, 996803956, 1650557963, 2455263465, 1615657190, 1706712446, 198838765, 96485762, 1811319715, 198449801, 896744931, 1000051995, 97879746, 1338173477, 1071253039, 185600907]\n"
          ]
        }
      ],
      "source": [
        "shingles_doc1 = [6840193950337781150, 1695454882927409067, 7334696573069238721]\n",
        "minhasher = MinHashing()\n",
        "signature = minhasher.ComputeSignatures(shingles_doc1)\n",
        "\n",
        "print(signature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LRWsdNSFn2_v"
      },
      "outputs": [],
      "source": [
        "class CompareSignatures():\n",
        "  @staticmethod\n",
        "  def compare(sig1,sig2):\n",
        "    if len(sig1) != len(sig2):\n",
        "      raise ValueError(\"Signatures must have the same length\")\n",
        "\n",
        "    matches = sum(1 for a,b in zip(sig1,sig2) if a == b)\n",
        "    return matches /len(sig1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg9Oi9uk7EMl"
      },
      "source": [
        "**Test à part**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4db9hde5_8P",
        "outputId": "0f944883-e6d8-48cc-ec66-1506725268d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pairs with MinHash similarity >= 0.6:\n",
            "doc1.txt ~ doc2.txt : 1.00\n"
          ]
        }
      ],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import col\n",
        "# import mmh3\n",
        "# from itertools import combinations\n",
        "\n",
        "# # --- Spark session ---\n",
        "# spark = SparkSession.builder.master(\"local[*]\").appName(\"Test MinHash\").getOrCreate()\n",
        "\n",
        "# # --- Exemples de textes ---\n",
        "# docs = [\n",
        "#     (\"doc1.txt\", \"The quick brown fox jumps over the lazy dog.\"),\n",
        "#     (\"doc2.txt\", \"The quick brown fox jumps over the lazy dog!\"),  # très similaire à doc1\n",
        "#     (\"doc3.txt\", \"Alice was beginning to get very tired of sitting by her sister on the bank.\"),\n",
        "#     (\"doc4.txt\", \"It was the best of times, it was the worst of times.\"),\n",
        "#     (\"doc5.txt\", \"Mr. Utterson, the lawyer, was a man of a rugged countenance.\")\n",
        "# ]\n",
        "\n",
        "# # --- Création du DataFrame Spark ---\n",
        "# df_docs = spark.createDataFrame(docs, [\"filename\", \"text\"])\n",
        "\n",
        "# # --- Shingling (comme ta classe Shingling) ---\n",
        "# shingler = Shingling(k=3)  # 3 mots par shingle pour exemple\n",
        "# df_shingles = shingler.transform(df_docs, \"text\")\n",
        "\n",
        "# # --- MinHashing ---\n",
        "# n_hashes = 50\n",
        "# minhasher = MinHashing(n_hashes)\n",
        "# df_shingles = df_shingles.withColumn(\"minhash\", minhasher.minhash_udf()(col(\"hashed_shingles\")))\n",
        "\n",
        "# # --- Comparaison MinHash ---\n",
        "# threshold_minhash = 0.6\n",
        "# rows = df_shingles.select(\"filename\", \"minhash\").collect()\n",
        "# similar_pairs_minhash = []\n",
        "\n",
        "# for r1, r2 in combinations(rows, 2):\n",
        "#     sim = CompareSignatures.compare(r1[\"minhash\"], r2[\"minhash\"])\n",
        "#     if sim >= threshold_minhash:\n",
        "#         similar_pairs_minhash.append((r1[\"filename\"], r2[\"filename\"], sim))\n",
        "\n",
        "# print(\"Pairs with MinHash similarity >= 0.6:\")\n",
        "# for f1, f2, sim in similar_pairs_minhash:\n",
        "#     print(f\"{f1} ~ {f2} : {sim:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UapcL4zOnxgr"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "class LSH():\n",
        "  def __init__(self,num_bands=20,num_rows_band=5):\n",
        "    self.num_bands = num_bands\n",
        "    self.num_rows_band = num_rows_band\n",
        "    self.buckets = [defaultdict(list) for _ in range(num_bands)]\n",
        "\n",
        "  def band_hash(self,band):\n",
        "    return mmh3.hash(str(band),signed = False)\n",
        "  \n",
        "  def index(self, doc_signatures, doc_names):\n",
        "    for doc_id,sig in zip(doc_names,doc_signatures):\n",
        "      for b in range(self.num_bands):\n",
        "        start = b*self.num_rows_band\n",
        "        end = b + self.num_rows_band\n",
        "        band = tuple(sig[start:end])\n",
        "        h = self.band_hash(band)\n",
        "        self.buckets[b][h].append(doc_id)\n",
        "\n",
        "  def candidate_pairs(self):\n",
        "    \"\"\"\n",
        "    Retourne la liste des paires candidates (documents qui tombent dans au moins un même bucket)\n",
        "    \"\"\"\n",
        "    candidates = set()\n",
        "    for b in range(self.num_bands):\n",
        "      for bucket_docs in self.buckets[b].values():\n",
        "        if len(bucket_docs) > 1:\n",
        "                    # Toutes les combinaisons de documents dans le bucket\n",
        "          for pair in combinations(bucket_docs, 2):\n",
        "            candidates.add(tuple(sorted(pair)))\n",
        "    return list(candidates)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (venv)",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
